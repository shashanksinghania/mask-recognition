{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\shashank\\anaconda3\\lib\\site-packages (4.3.0.36)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from opencv-python) (1.18.1)\n",
      "Collecting pytest-shutil\n",
      "  Downloading pytest_shutil-1.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest-shutil) (1.14.0)\n",
      "Collecting path.py\n",
      "  Downloading path.py-12.4.0-py3-none-any.whl (2.3 kB)\n",
      "Requirement already satisfied: pytest in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest-shutil) (5.3.5)\n",
      "Requirement already satisfied: contextlib2 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest-shutil) (0.6.0.post1)\n",
      "Requirement already satisfied: mock in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest-shutil) (4.0.1)\n",
      "Collecting execnet\n",
      "  Downloading execnet-1.7.1-py2.py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest-shutil) (1.1.0)\n",
      "Requirement already satisfied: path<13.2 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from path.py->pytest-shutil) (13.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (1.8.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (20.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (19.3.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (0.1.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (1.5.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from pytest->pytest-shutil) (0.4.3)\n",
      "Collecting apipkg>=1.4\n",
      "  Downloading apipkg-1.5-py2.py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from packaging->pytest->pytest-shutil) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\shashank\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.12->pytest->pytest-shutil) (2.2.0)\n",
      "Installing collected packages: path.py, apipkg, execnet, pytest-shutil\n",
      "Successfully installed apipkg-1.5 execnet-1.7.1 path.py-12.4.0 pytest-shutil-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install pytest-shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import random\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of images in our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images with mask used:  692\n",
      "Number of images without mask used:  686\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of images with mask used: \", len(os.listdir('../dataset/with_mask')))\n",
    "print(\"Number of images without mask used: \", len(os.listdir('../dataset/without_mask')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(source, train, test, test_size):\n",
    "    data = os.listdir(source)\n",
    "    train_size = int(len(data)*(1 - test_size))\n",
    "    shuffled_data = random.sample(data, len(data))\n",
    "    training_data = shuffled_data[:train_size]\n",
    "    testing_data = shuffled_data[train_size:]\n",
    "    \n",
    "    # make new folders train and test\n",
    "    for img in training_data:\n",
    "        temp_image = source+img\n",
    "        train_image = train+img\n",
    "        copyfile(temp_image, train_image)\n",
    "    \n",
    "    for img in testing_data:\n",
    "        temp_image = source+img\n",
    "        test_image = test+img\n",
    "        copyfile(temp_image, test_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_SOURCE = '../dataset/with_mask/'\n",
    "TRAIN_MASK_SOURCE = '../dataset/train/yes/'\n",
    "TEST_MASK_SOURCE = '../dataset/test/yes/'\n",
    "\n",
    "NO_MASK_SOURCE = '../dataset/without_mask/'\n",
    "TEST_NO_MASK_SOURCE = '../dataset/test/no/'\n",
    "TRAIN_NO_MASK_SOURCE = '../dataset/train/no/'\n",
    "\n",
    "split_data(MASK_SOURCE, TRAIN_MASK_SOURCE, TEST_MASK_SOURCE, 0.2)\n",
    "split_data(NO_MASK_SOURCE, TRAIN_NO_MASK_SOURCE, TEST_NO_MASK_SOURCE, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training with mask:  553\n",
      "Number of training without mask:  548\n",
      "Number of testing with mask:  139\n",
      "Number of testing without mask:  138\n"
     ]
    }
   ],
   "source": [
    "print('Number of training with mask: ', len(os.listdir(TRAIN_MASK_SOURCE)))\n",
    "print('Number of training without mask: ', len(os.listdir(TRAIN_NO_MASK_SOURCE)))\n",
    "print('Number of testing with mask: ', len(os.listdir(TEST_MASK_SOURCE)))\n",
    "print('Number of testing without mask: ', len(os.listdir(TEST_NO_MASK_SOURCE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1101 images belonging to 2 classes.\n",
      "Found 277 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = '../dataset/train/'\n",
    "TEST_DIR = '../dataset/test/'\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(TRAIN_DIR,\n",
    "                                                 target_size = (150,150),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(TEST_DIR,\n",
    "                                            target_size = (150,150),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', input_shape=[150, 150, 3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to get best weights with monitored with validation loss\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model', monitor='val_loss', verbose=0, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "35/35 [==============================] - 93s 3s/step - loss: 0.9161 - acc: 0.5649 - val_loss: 0.3634 - val_acc: 0.9025\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.3626 - acc: 0.8538 - val_loss: 0.1423 - val_acc: 0.9603\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.3132 - acc: 0.8765 - val_loss: 0.1264 - val_acc: 0.9531\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.2998 - acc: 0.8910 - val_loss: 0.1211 - val_acc: 0.9495\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.2742 - acc: 0.9019 - val_loss: 0.1049 - val_acc: 0.9639\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 58s 2s/step - loss: 0.1896 - acc: 0.9273 - val_loss: 0.0911 - val_acc: 0.9747\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.1856 - acc: 0.9273 - val_loss: 0.1425 - val_acc: 0.9422\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.1881 - acc: 0.9228 - val_loss: 0.0941 - val_acc: 0.9639\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.1498 - acc: 0.9437 - val_loss: 0.0944 - val_acc: 0.9603\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 57s 2s/step - loss: 0.1432 - acc: 0.9419 - val_loss: 0.0749 - val_acc: 0.9711\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.1508 - acc: 0.9428 - val_loss: 0.0893 - val_acc: 0.9675\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.1800 - acc: 0.9282 - val_loss: 0.0992 - val_acc: 0.9567\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.1536 - acc: 0.9464 - val_loss: 0.1072 - val_acc: 0.9531\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 50s 1s/step - loss: 0.1285 - acc: 0.9500 - val_loss: 0.0828 - val_acc: 0.9675\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.1274 - acc: 0.9537 - val_loss: 0.0861 - val_acc: 0.9675\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.1053 - acc: 0.9637 - val_loss: 0.0733 - val_acc: 0.9783\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.1059 - acc: 0.9619 - val_loss: 0.0885 - val_acc: 0.9675\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.1302 - acc: 0.9591 - val_loss: 0.0668 - val_acc: 0.9783\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.1307 - acc: 0.9555 - val_loss: 0.1164 - val_acc: 0.9567\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.0993 - acc: 0.9691 - val_loss: 0.0822 - val_acc: 0.9675\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.0902 - acc: 0.9637 - val_loss: 0.1436 - val_acc: 0.9495\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 56s 2s/step - loss: 0.0831 - acc: 0.9619 - val_loss: 0.0924 - val_acc: 0.9675\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 55s 2s/step - loss: 0.1320 - acc: 0.9546 - val_loss: 0.0742 - val_acc: 0.9711\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.1143 - acc: 0.9609 - val_loss: 0.0655 - val_acc: 0.9783\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 54s 2s/step - loss: 0.0741 - acc: 0.9773 - val_loss: 0.0603 - val_acc: 0.9819\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 60s 2s/step - loss: 0.0543 - acc: 0.9791 - val_loss: 0.0547 - val_acc: 0.9892\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 52s 1s/step - loss: 0.0854 - acc: 0.9728 - val_loss: 0.0830 - val_acc: 0.9675\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.1446 - acc: 0.9491 - val_loss: 0.0515 - val_acc: 0.9892\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 51s 1s/step - loss: 0.1028 - acc: 0.9646 - val_loss: 0.1272 - val_acc: 0.9531\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 53s 2s/step - loss: 0.0725 - acc: 0.9755 - val_loss: 0.0452 - val_acc: 0.9928\n"
     ]
    }
   ],
   "source": [
    "model = cnn.fit(x = training_set, validation_data = test_set, epochs = 30, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# To save the model in your disk\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = cnn.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "cnn.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to directly load the trained model if you don't wish to train it on your computer\n",
    "'''\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
